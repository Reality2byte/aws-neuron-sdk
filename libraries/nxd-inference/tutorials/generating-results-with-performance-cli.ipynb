{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b8b4883",
   "metadata": {},
   "source": [
    "# Tutorial: Evaluating Performance of Llama-3.3-70B on Neuron using Performance CLI\n",
    "\n",
    "## Introduction\n",
    "This tutorial provides a step-by-step guide to measure the performance of Llama3.3 70B on `Trn1` with easy to reproduce benchmarks.\n",
    "\n",
    "In this tutorial you will learn how llama-3.3-70B can be easily tested with llm-perf for 3.3-70b-instruct model.\n",
    "\n",
    "You must have the instruction-tuned version of llama-3.3 70b [meta-llama/Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/) available for Hugging Face to successfully complete it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f747ff61",
   "metadata": {},
   "source": [
    "## Environment Setup Guide\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "This tutorial requires that you have a `Trn1` instance created from a Deep Learning AMI that has the Neuron SDK pre-installed. This tutorial depends on the Neuron fork of vLLM.\n",
    "\n",
    "Before running evaluations, ensure your environment is properly configured by following these essential setup guides:\n",
    "\n",
    "1. [NxD Inference Setup Guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/nxdi-setup.html)\n",
    "2. [vLLM User Guide for NxD Inference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/developer_guides/vllm-user-guide.html)\n",
    "\n",
    "###  Installing dependencies\n",
    "\n",
    "- Copy the [inference-benchmarking](https://github.com/aws-neuron/aws-neuron-samples/tree/master/inference-benchmarking/) directory to some location on your instance. \n",
    "- Change your current working directory to your copy of [inference-benchmarking](https://github.com/aws-neuron/aws-neuron-samples/tree/master/inference-benchmarking/). \n",
    "- Install other required dependencies in the same Python env (such as `aws_neuron_venv_pytorch`, if you followed the steps in [Manually install NxD Inference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/nxdi-setup.html#id3)) by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d51cd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws-neuron-llm-evaluation                1.0\n",
      "awsneuroneval                            1.0\n",
      "libneuronxla                             2.2.7366.0+1faf0ddf\n",
      "neuron-torch-tools                       1.0.0.33853+83b6bf63a\n",
      "neuronx-cc                               2.20.2831.0+8bfecb25\n",
      "neuronx-cc-devel                         2.20.2831.0+8bfecb25\n",
      "neuronx-distributed                      0.14.17095+c66a8ca6\n",
      "neuronx-distributed-inference            0.5.0+dev\n",
      "torch-neuronx                            2.7.0.2.9.8707+08e1f40d\n",
      "vllm-neuronx                             0.9.0.dev0+neuron225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws-neuronx-collectives/now 2.27.13.0-f3bd841a2 amd64 [installed,local]\n",
      "aws-neuronx-dkms/now 2.23.0.0 all [installed,local]\n",
      "aws-neuronx-runtime-lib/now 2.27.7.0-765d5f599 amd64 [installed,local]\n",
      "aws-neuronx-tools/now 2.25.100.0 amd64 [installed,local]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/aws_neuron_venv/lib/python3.10/site-packages/IPython/core/completerlib.py:371: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bks = self.db.get('bookmarks',{})\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip list | grep neuron\n",
    "apt list --installed | grep neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87a5da4",
   "metadata": {},
   "source": [
    "You should see Neuron packages including `neuronx-distributed-inference` and its related components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186aacdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4db09b0",
   "metadata": {},
   "source": [
    "## Download llama-3.3 70B\n",
    "To use this sample, you must first download [meta-llama/Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) model checkpoint from Hugging Face `/home/ubuntu/models/Llama-3.3-70B-Instruct/` on the `Trn1` instance. For more information, see [Downloading models](https://huggingface.co/docs/hub/en/models-downloading) in the Hugging Face documentation.\n",
    "\n",
    "To use a Jupyter Notebook on the Neuron instance, follow this [guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/setup/notebook/setup-jupyter-notebook-steps-troubleshooting.html).\n",
    "\n",
    "### Running Evaluations\n",
    "There are two methods that you can use to run your evaluation.\n",
    "\n",
    "1. Use a YAML configuration file and `performance.py` script\n",
    "\n",
    "2. Write your own python script that uses several components provided in `performance.py` and `server_config.py`\n",
    "\n",
    "Each use case is demonstrated below:\n",
    "\n",
    "### 1. Running performance with yaml config file\n",
    "In this method, you create  a YAML (`.yaml`) config file that specifies the server configuration and testing scenario you want to run. Create `config.yaml` with the following content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b648ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd inference-benchmarking/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32fab921",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install requirements present in inference-benchmarking package\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4702a430",
   "metadata": {},
   "source": [
    "perf.yaml\n",
    "```yaml\n",
    "server:\n",
    "  name: \"llama-3.3-70b-instruct\"\n",
    "  model_path: \"/home/ubuntu/models/llama-3.3-70b-instruct\"\n",
    "  model_s3_path: null\n",
    "  compiled_model_path: \"/home/ubuntu/traced_models/llama-3.3-70b-instruct\"\n",
    "  max_seq_len: 256\n",
    "  context_encoding_len: 128\n",
    "  tp_degree: 32\n",
    "  server_port: 8000\n",
    "  continuous_batch_size: 1\n",
    "  custom_chat_template_path: \"default\"\n",
    "\n",
    "test:\n",
    "  performance:\n",
    "    sonnets_small_test:\n",
    "      client: \"llm_perf\"\n",
    "      client_type: \"llm_perf_github_patched\"\n",
    "      n_batches: 1\n",
    "      max_concurrent_requests: 20\n",
    "      timeout: 3600\n",
    "      input_size: 128\n",
    "      output_size: 124\n",
    "      client_params:\n",
    "        stddev_input_tokens: 0\n",
    "        stddev_output_tokens: 1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad27ecd",
   "metadata": {},
   "source": [
    "The above YAML file is explained in more detail in [Performance Params guide](../developer_guides/performance-cli-params.html)\n",
    "\n",
    "\n",
    "For changing sequence length you must adjust `max_seq_len`. \n",
    "\n",
    "Run `python performance.py --config perf.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python performance.py --config perf.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dad77c",
   "metadata": {},
   "source": [
    "### 2. Running perf as part of your own Python code\n",
    "\n",
    "You nmight want to run the performance script as part of your Python code. For example, you might want to change the configuration programatically or post-process the results. This is possible using 3 main components provided in `performance.py` and `server_config.py`.\n",
    "\n",
    "1. Server Configuration: Use ServerConfig to define the vLLM server settings\n",
    "\n",
    "2. Performance Scenario: Use PerformanceScenario to specify evaluation parameters\n",
    "\n",
    "3. Test Execution: Run the performance with the configured settings\n",
    "\n",
    "### Step-by-Step Implementation\n",
    "First, import the necessary components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ac8e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd \"/home/ubuntu/inference-benchmarking\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7d4bd54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from performance import PerformanceScenario, run_perf_test\n",
    "from server_config import ServerConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263b8270",
   "metadata": {},
   "source": [
    "### 1. Configure the Server\n",
    "\n",
    "Set up your server configuration with ServerConfig. This example uses Llama 3.3-70b Instruct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e37832b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"llama-3.3-70b-instruct\"\n",
    "server_config = ServerConfig(\n",
    "    name=name,\n",
    "    model_path=f\"/home/ubuntu/models/{name}\",  # Local model path\n",
    "    model_s3_path=None,  # S3 model path\n",
    "    max_seq_len=256,          # Maximum sequence length\n",
    "    context_encoding_len=128,  # Context window size\n",
    "    tp_degree=32,               # Tensor parallel degree\n",
    "    n_vllm_threads=1,          # Number of vLLM threads\n",
    "    server_port=8000,           # Server port\n",
    "    continuous_batch_size=1,    # Batch size for continuous batching\n",
    "    custom_chat_template_path=\"default\" # Chat template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aecfe7",
   "metadata": {},
   "source": [
    "### 2. Define Performance Scenarios\n",
    "\n",
    "Create a PerformanceScenario to specify your perf parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "818598ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = PerformanceScenario(\n",
    "    client=\"llm_perf\",          # Evaluation client\n",
    "    client_type=\"llm_perf_github_patched\",\n",
    "    n_batches=1,\n",
    "    max_concurrent_requests=20,  # Maximum concurrent requests\n",
    "    timeout=5000,              # Timeout in seconds - changed to 5000 from 3600\n",
    "    input_size=128,            # Input length\n",
    "    output_size=124,           # Output length\n",
    "    client_params={\"stddev_input_tokens\": 0, \"stddev_output_tokens\": 1}  # Client-specific parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57a53d0",
   "metadata": {},
   "source": [
    "### 3. Run the Evaluation\n",
    "\n",
    "Execute the evaluation using `run_perf_test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c5dcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the test with a named scenario\n",
    "results_collection = run_perf_test(\n",
    "    server_config=server_config,\n",
    "    named_scenarios={\"mytest\": scenario}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ebe3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# Display results\n",
    "pprint(results_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8753980b",
   "metadata": {},
   "source": [
    "This code will execute and return detailed performance metrics for the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuron_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
